
# ğŸš€ Pipeline de Datos con Apache Airflow & PostgreSQL

  

> **Proyecto de IngenierÃ­a de Datos | ETL Pipeline | Data Pipeline | Data Engineering**

  

[![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-017CEE?style=for-the-badge&logo=Apache%20Airflow&logoColor=white)](https://airflow.apache.org/)

[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-316192?style=for-the-badge&logo=postgresql&logoColor=white)](https://www.postgresql.org/)

[![Apache Superset](https://img.shields.io/badge/Apache%20Superset-FF6B6B?style=for-the-badge&logo=apache&logoColor=white)](https://superset.apache.org/)

[![Docker](https://img.shields.io/badge/Docker-2CA5E0?style=for-the-badge&logo=docker&logoColor=white)](https://www.docker.com/)

[![Python](https://img.shields.io/badge/Python-FFD43B?style=for-the-badge&logo=python&logoColor=blue)](https://www.python.org/)

  

## ğŸ¯ DescripciÃ³n del Proyecto

  

Este proyecto demuestra un **pipeline de datos completo** implementado con las mejores prÃ¡cticas de **Data Engineering**. Incluye la automatizaciÃ³n de procesos ETL (Extract, Transform, Load) utilizando Apache Airflow para orquestar la carga y transformaciÃ³n de datos en PostgreSQL, y Apache Superset para la visualizaciÃ³n y anÃ¡lisis de datos.

  

### ğŸ”¥ CaracterÃ­sticas Destacadas

  

-  **ğŸ”„ OrquestaciÃ³n Automatizada**: DAGs de Airflow para pipeline de datos

-  **ğŸ“Š Modelado de Datos**: CreaciÃ³n de tablas dimensionales y hechos

-  **ğŸ“ˆ Dashboard Interactivo**: VisualizaciÃ³n con Apache Superset

-  **ğŸ³ ContainerizaciÃ³n**: ImplementaciÃ³n con Docker para fÃ¡cil despliegue

-  **ğŸ“ˆ Pipeline ETL**: Proceso completo de extracciÃ³n, transformaciÃ³n y carga

-  **ğŸ”’ GestiÃ³n de Conexiones**: ConfiguraciÃ³n segura de bases de datos

-  **ğŸ“‹ Monitoreo**: Logs y seguimiento de ejecuciÃ³n de tareas

  

## ğŸ—ï¸ Arquitectura del Sistema

  

```

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚ Sample Data â”‚â”€â”€â”€â–¶â”‚ Apache Airflow â”‚â”€â”€â”€â–¶â”‚ PostgreSQL â”‚â”€â”€â”€â–¶â”‚ Apache Superset â”‚

â”‚ (CSV Files) â”‚ â”‚ (ETL Engine) â”‚ â”‚ (Data Lake) â”‚ â”‚ (Dashboard) â”‚

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”‚ â”‚ â”‚

â–¼ â–¼ â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”

â”‚ DAGs â”‚ â”‚ OBT â”‚ â”‚ Charts & â”‚

â”‚ ETL â”‚ â”‚ Tables â”‚ â”‚ Dashboards â”‚

â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

  

## ğŸ› ï¸ Stack TecnolÃ³gico

  

| Componente | TecnologÃ­a | VersiÃ³n | PropÃ³sito |

|------------|------------|---------|-----------|

| **Orquestador** | Apache Airflow | 2.3.0 | ProgramaciÃ³n y monitoreo de pipelines |

| **Base de Datos** | PostgreSQL | Latest | Almacenamiento de datos |

| **Dashboard** | Apache Superset | Latest | VisualizaciÃ³n y anÃ¡lisis de datos |

| **ContainerizaciÃ³n** | Docker & Docker Compose | 3.8 | Despliegue y escalabilidad |

| **Lenguaje** | Python | 3.8 | LÃ³gica de transformaciÃ³n |

| **GestiÃ³n de Datos** | SQL | - | Consultas y transformaciones |

  

## ğŸ“ Estructura del Proyecto

  

```

â”œâ”€â”€ airflow/

â”‚ â””â”€â”€ dags/

â”‚ â””â”€â”€ initialise_data.py # ğŸ¯ DAG principal de ETL

â”œâ”€â”€ dbt/

â”‚ â””â”€â”€ sql/

â”‚ â””â”€â”€ create_and_populate_obt_orders.sql # ğŸ”„ Transformaciones SQL

â”œâ”€â”€ sample_data/ # ğŸ“Š Datos de entrada (CSV)

â”œâ”€â”€ docker-compose.yml # ğŸ³ OrquestaciÃ³n de servicios

â”œâ”€â”€ dockerfile # ğŸ³ Imagen personalizada de Airflow

â””â”€â”€ scripts_airflow/ # âš™ï¸ Scripts de inicializaciÃ³n

```

  

## ğŸš€ CÃ³mo Ejecutar

  

### Prerrequisitos

- Docker Desktop

- Docker Compose

- 8GB RAM disponible

  

### Pasos de EjecuciÃ³n

  

```bash

# 1. Clonar el repositorio

git  clone  <repository-url>

cd  dbt-airflow-docker-compose-master

  

## ğŸ¯ **Inicio AutomÃ¡tico (Recomendado)**

```bash

# Hacer ejecutable el script

chmod +x start_project.sh

  

# Ejecutar inicio automÃ¡tico

./start_project.sh

```

  

## ğŸ“‹ **Inicio Manual**

```bash

# 1. Crear red Docker (si no existe)

docker  network  create  superset-network

  

# 2. Levantar los servicios

docker-compose  up  -d

  

# 3. Acceder a los servicios

# ğŸŒ Airflow: http://localhost:8000

# ğŸ‘¤ Usuario: admin | ğŸ”‘ ContraseÃ±a: admin

# ğŸŒ Superset: http://localhost:8088/

# ğŸ‘¤ Usuario: admin | ğŸ”‘ ContraseÃ±a: admin

  

# 4. Ejecutar el DAG

# El DAG '1_load_initial_data' se ejecutarÃ¡ automÃ¡ticamente

```

  

## âœ… **VerificaciÃ³n del Sistema**

  

### **1. Verificar Servicios**

```bash

docker-compose  ps

```

  

### **2. Verificar Logs de Airflow**

```bash

docker-compose  logs  airflow

```

  

### **3. Verificar Conexiones**

- Ve a Airflow: http://localhost:8000

- Admin â†’ Connections

- DeberÃ­as ver: `dbt_postgres_instance_raw_data`

  

### **4. Ejecutar DAG Manualmente**

- En Airflow, ve al DAG `1_load_initial_data`

- Haz clic en "Play" para ejecutarlo manualmente

- Verifica que todas las tareas se completen exitosamente

  

### **5. Verificar en Superset**
- Ejecutar docker compose -f docker-compose-image-tag.yml up

- Ve a Superset: http://localhost:8088

- Conecta la base de datos usando la cadena: `postgresql+psycopg2://dbtuser:pssd@postgres-dbt:5432/dbtdb`

- Verifica que puedas ver las tablas creadas y juega con los datos ğŸ˜ƒ

```
![Superset](dbt-airflow-docker-compose-master/bi.png)
  

## ğŸ”— Conexiones y URLs

  

### Airflow

- **URL**: http://localhost:8000

- **Usuario**: `admin`

- **ContraseÃ±a**: `admin`

  

### Apache Superset

- **URL**: http://localhost:8088/

- **Usuario**: `admin`

- **ContraseÃ±a**: `admin`

  

### Base de Datos PostgreSQL

- **Host**: `postgres-dbt`

- **Puerto**: `5432`

- **Base de Datos**: `dbtdb`

- **Usuario**: `dbtuser`

- **ContraseÃ±a**: `pssd`

- **Cadena de ConexiÃ³n**: `postgresql+psycopg2://dbtuser:pssd@postgres-dbt:5432/dbtdb`

  

## ğŸ“Š Modelo de Datos

  

### Tablas de Origen (Raw Data)

- **`business_types`** - Tipos de negocio y categorÃ­as

- **`customers`** - InformaciÃ³n de clientes y metadata

- **`orders`** - Transacciones y pedidos

- **`currency_codes`** - Tasas de cambio y monedas

- **`site_codes`** - Ubicaciones geogrÃ¡ficas

  

### Tabla de Destino (Analytics) - OBT

- **`obt_orders`** - **One Big Table** consolidada para anÃ¡lisis

  

#### ğŸ¯ **CaracterÃ­sticas de la Tabla OBT (One Big Table)**

  

La tabla `obt_orders` es el corazÃ³n del modelo de datos, implementando el patrÃ³n **One Big Table** que ofrece:

  

- **ğŸ“Š ConsolidaciÃ³n Completa**: Todos los datos de negocio en una sola tabla

- **ğŸ”— Joins Pre-calculados**: Relaciones entre entidades ya resueltas

- **ğŸ“ˆ MÃ©tricas Agregadas**: KPIs calculados automÃ¡ticamente

- **ğŸŒ Enriquecimiento GeogrÃ¡fico**: Datos de ubicaciÃ³n integrados

- **ğŸ’° InformaciÃ³n Monetaria**: MÃºltiples monedas con tasas de cambio

- **ğŸ“… Dimensiones Temporales**: Fechas con atributos calculados

  

#### **Estructura de la OBT:**

```sql

-- Campos principales de la tabla OBT

order_id, order_date, delivery_date, delivery_day_of_week, delivery_month, delivery_quarter,

customer_id, customer_name, email_address, business_type_id, business_type_name,

is_key_account, customer_archived,

site_code, city_name, country_name, continent,

order_status, total, total_shipping, currency_ars, currency_mxn, currency_cop, currency_brl,

tracking_code, gmv_enabled, order_number, shipping_by_tracking,

latitude, longitude

```

  

## ğŸ”„ Pipeline ETL

  

1.  **ğŸ“¥ Extract**: Carga de archivos CSV desde `sample_data/`

2.  **ğŸ”„ Transform**: CreaciÃ³n de esquemas y tablas con lÃ³gica de negocio

3.  **ğŸ“¤ Load**: InserciÃ³n de datos transformados en tablas analÃ­ticas

4.  **âœ… Validate**: VerificaciÃ³n de integridad y calidad de datos

5.  **ğŸ“Š Visualize**: CreaciÃ³n de dashboards en Apache Superset

  

## ğŸ“ˆ Casos de Uso

  

-  **AnÃ¡lisis de Ventas**: Seguimiento de pedidos y rendimiento

-  **Inteligencia de Negocio**: Dashboards y reportes ejecutivos

-  **AnÃ¡lisis GeogrÃ¡fico**: DistribuciÃ³n de clientes por ubicaciÃ³n

-  **MÃ©tricas de Negocio**: KPIs e indicadores de rendimiento

-  **AnÃ¡lisis Temporal**: Tendencias por dÃ­a, mes y trimestre

-  **SegmentaciÃ³n de Clientes**: AnÃ¡lisis por tipo de negocio y cuenta clave

  

### Conexiones de Base de Datos

-  **Airflow DB**: PostgreSQL para metadatos de Airflow

-  **Data DB**: PostgreSQL para datos de negocio

-  **Superset**: ConexiÃ³n a PostgreSQL para visualizaciones

 
  

## ğŸŒŸ PrÃ³ximos Pasos

  

- [ ] Implementar tests de calidad de datos

- [ ] Agregar alertas y notificaciones

- [ ] Crear dashboards de monitoreo

- [ ] Implementar CI/CD para el pipeline

- [ ] Agregar mÃ¡s transformaciones de datos

- [ ] Implementar cache de Superset para mejor rendimiento

- [ ] Agregar autenticaciÃ³n LDAP/SSO

  

## ğŸ”§ SoluciÃ³n de Problemas

  

Si encuentras el error `The conn_id 'dbt_postgres_instance_raw_data' isn't defined`, reinicia los servicios con `docker-compose restart`.

  

### ğŸš€ SoluciÃ³n RÃ¡pida

```bash

# Ejecutar script de reparaciÃ³n automÃ¡tica

docker-compose  restart

```

  

### ğŸ“‹ VerificaciÃ³n RÃ¡pida

```bash

# Verificar estado de contenedores

docker-compose  ps

  

# Verificar conexiones en Airflow

docker-compose  exec  airflow  airflow  connections  list

``
